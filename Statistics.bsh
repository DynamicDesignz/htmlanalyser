:

init() {

	[ -d analysis ] && rm -rf analysis
	mkdir analysis
	[ -d downloads ] && rm -rf downloads
	mkdir downloads

}

remove() {

  	[ -d downloads ] && rm -rf downloads
	mkdir downloads

}

analyseLinks() {

	grep '<a' $FILENAME | grep -o 'href= h.*' > href.txt

	touch links.txt

	while read line; do
		set -- $( echo $line )
		echo $2 >> links.txt 
	done < href.txt

	sort -u links.txt > uniqueLinks.txt
	rm links.txt

	touch sameDomain.txt

	while read url; do

		URL=$( echo $url )
		extractDomain
		set -- $( head -1 temp1 )
		THISDOMAIN=$( echo $1 )
		rm temp1
		
		if [ "$( echo $DOMAIN )" == "$( echo $THISDOMAIN )" ]; then
			echo $URL >> sameDomain.txt
		fi

	done < uniqueLinks.txt

	rm uniqueLinks.txt
	sort -u sameDomain.txt > linksMasterListSorted.txt
	rm sameDomain.txt

}

downloadChildren() {

	for j in $(seq 1 $LEVEL); do
		
		CHILD=$( echo $j ) # Depth of download
		PARENT=$( expr $CHILD - 1 ) # Depth of analysis
		ITERATION=0
		
		for k in ${DOMAIN}_${PARENT}_?.html;  do
			
			FILENAME=$( echo $k )
			analyseLinks
			
			while read url; do
			
				URL=$( echo $url )

				if [ "$( grep ^$URL$ < visitedLinks.txt )" == "" ]; then
					
					ITERATION=$( expr $ITERATION + 1 ) 
					
					downloadPage

				fi

			done < linksMasterListSorted.txt
			
		done
		
	done
}

downloadPage() {

	curl -sS $URL | sed 's/'\''/ /g' | sed 's/'\"'/ /g' | tr '[A-Z]' '[a-z]' | sed 's/'\>\<'/\>\'$'\n\</g' > ${DOMAIN}_${CHILD}_${ITERATION}.html
	echo $URL >> visitedLinks.txt
	
}

extractDomain() { # $1 of set -- $( head -1 temp1) is domain name

	touch temp1
	echo $URL > temp1
	HTTP="https\?:\/\/w\{0,3\}"
	sed 's/'${HTTP}'//' temp1 | sed 's/'[.]'/ /g' > temp1 
}

downloadIndex() {

	curl -sS $URL | sed 's/'\''/ /g' | sed 's/'\"'/ /g' | tr '[A-Z]' '[a-z]' | sed 's/'\>\<'/\>\'$'\n\</g' > ${DOMAIN}_0_0.html
	touch visitedLinks.txt
	echo ${URL}/ >> visitedLinks.txt
	
}

findDomainAndMoveIn() {

	extractDomain
	set -- $( head -1 temp1 )
	DOMAIN=$( echo $1 ) # Define domain for analysis
	rm temp1
	[ ! -d "$DOMAIN" ] && mkdir ${DOMAIN}folder
	cd ${DOMAIN}folder
	
}

downloadManager() {

	URLS=$( head -1 urlArguments )
	set -- $URLS
	
	for i in $*; do 
	
		URL=$( echo $i )
		findDomainAndMoveIn
		downloadIndex
		[ $LEVEL -gt 0 ] && downloadChildren
		cd ..
		
	done
	
}

findGenerator() { # If GENERATOR = 1 then generator exists and use CONTENT

	grep "name= generator" < $FILENAME > temp1
	GENERATOR=$( grep "name= generator" | wc -l )
	[ $GENERATOR -eq 1 ] && CONTENT=$( set --$( grep -o 'content=.*' $FILENAME ) | echo $2 )
	# [ $( echo $CONTENT | wc -w) -gt 0 ] && CONTENT=$( sed "s/.*content='\(*\)'.*/\1/" )
	rm temp1
	
}

findGoogleAnalytics() { # If GOOGLEANALYTICS = 1 then google-analytics exists

	[ $( grep "https://www.google-analytics.com/urchin.js" < $FILENAME | wc -l ) -gt 0] && GOOGLEANALYTICS=1
	
}

findExternalScripts() {

	EXTERNALSCRIPTCOUNT = $( grep 'src' SCRIPTS | wc -l )

}

findInternalScripts() {

	INTERNALSCRIPTCOUNT = $( grep -v '<script.*src.*>' SCRIPTS | grep '<script' )

}

findScripts() { # SCRIPTS indicates number of script tags

	if [ $( SCRIPTS = $( grep 'script' < $FILENAME | grep -v '</script>' ) ) -gt 0 ]; then  
		SCRIPTCOUNT = $( echo SCRIPTS | wc -l )
 		findExternalScripts 
		findInternalScripts
	fi
}

findExternalCss() {

	EXTERNALCSSCOUNT = $( grep 'stylesheet' $FILENAME | wc -l )

}

findInternalCss() {

	INTERNALCSSCOUNT = $( grep '<style' centralop_0_0.html | wc -l )

}

findInlineCss() {

	INLINECSSCOUNT = $( grep -o 'style=' centralop_0_0.html | wc -l )

}

findMeta() {

	METACOUNT = $( grep '<meta' centralop_0_0.html | wc -l )

}

findIFrame() {

	IFRAMECOUNT = $( grep '<iframe' centralop_0_0.html | wc -l )

}


analyseHeader() {
		
	cd ..
	cd downloads
	findDomainAndMoveIn
	
	# Analyse all downloaded files in folder
	ls ${DOMAIN}* > pagesToAnalyse
	
	while read page; do
	
		findGenerator
		findGoogleAnalytics
		findScripts
		findExternalCss
		findInternalCss 
		findInlineCss
		findMeta
		findIFrame
		
	done < pagesToAnalyse
	
}

analyseTags() {

	grep -o '<[a-z].*>' centralop_0_0.html > tags.txt
	while read tag; do
		set -- $tag
		echo $1 >> tagsUnsorted.txt
	done < tags.txt 
	sed 's/[<>]/ /g' tagsUnsorted.txt > tagsSpace.txt
	while read tag; do
		set -- $tag
		echo $1 >> tagsSorted.txt
	done < tagsSpace.txt
	sort -u tagsSorted.txt > uniqueTagsSorted.txt
	
}

# analyseTags() {

	# set -- $TAGS
	# for i in $*; do
		# # Analyse Ã  faire
	# done
	
# }

# analyse() 

	# set -- $URLS
	
	# for i in $*; do
		
		# URL=$( echo $i )
		# analyseHeader
		# analyseTags
		
	# done
	
# }

# create() {

# }


case $1 in
-z) init;;
-c) remove;;
-[0-9]) LEVEL=$( echo $1 | tail -c 2 ); shift; cd downloads; touch urlArguments; echo $* > urlArguments; downloadManager;;
-a) shift; cd analysis; touch tags; echo $* > tags; analyse;;
-h) create;;
-l) analyseLinks;;
esac
