:

init() {

	[ -d downloads ] && rm -rf downloads
	mkdir downloads
	cd downloads
	[ -d analysedPages ] && rm -rf analysedPages
	mkdir analysedPages
	cd ..

}

remove() {

  	init

}

analyseLinks() {

	grep '<a' $FILENAME | grep -o 'href= h.*' > href.txt

	touch links.txt

	while read line; do
		set -- $( echo $line )
		echo $2 >> links.txt 
	done < href.txt

	rm href.txt

	sort -u links.txt > uniqueLinks.txt
	rm links.txt

	touch sameDomain.txt

	while read url; do

		URL=$( echo $url )
		extractDomain
		set -- $( head -1 temp1 )
		THISDOMAIN=$( echo $1 )
		rm temp1
		
		if [ "$( echo $DOMAIN )" == "$( echo $THISDOMAIN )" ]; then
			echo $URL >> sameDomain.txt
		fi

	done < uniqueLinks.txt

	rm uniqueLinks.txt
	sort -u sameDomain.txt > linksMasterListSorted.txt
	rm sameDomain.txt

}

downloadChildren() {

	for j in $(seq 1 $LEVEL); do
		
		CHILD=$( echo $j ) # Depth of download
		PARENT=$( expr $CHILD - 1 ) # Depth of analysis
		ITERATION=0
		
		for k in ${DOMAIN}_${PARENT}_?.html;  do
			
			FILENAME=$( echo $k )
			analyseLinks
			
			while read url; do
			
				URLCHILD=$( echo $url )

				if [ "$( grep ^$URLCHILD$ < visitedLinks.txt )" == "" ]; then
					
					ITERATION=$( expr $ITERATION + 1 ) 
					
					downloadPage

				fi

			done < linksMasterListSorted.txt
			
		done

		rm linksMasterListSorted.txt
		
	done
}

downloadPage() {

	curl -sS $URLCHILD | sed 's/'\''/ /g' | sed 's/'\"'/ /g' | tr '[A-Z]' '[a-z]' | sed 's/'\>\<'/\>\'$'\n\</g' > ${DOMAIN}_${CHILD}_${ITERATION}.html
	echo $URLCHILD >> visitedLinks.txt
	echo $URLCHILD >> ${DOMAIN}_${CHILD}_${ITERATION}.html # Stash the url here to facilitate analysis

}

extractDomain() { # $1 of set -- $( head -1 temp1 ) is domain name

	touch temp1
	echo $URL > temp1
	sed 's/http[s]\?:\/\/[w]\{0,3\}//g' temp1 > temp2
	sed 's/\./ /g' temp2 > temp1
	rm temp2

}

downloadIndex() {

	curl -sS $URL | sed 's/'\''/ /g' | sed 's/'\"'/ /g' | tr '[A-Z]' '[a-z]' | sed 's/'\>\<'/\>\'$'\n\</g' > ${DOMAIN}_0_0.html
	touch visitedLinks.txt
	echo ${URL}/ >> visitedLinks.txt
	echo $URL  >> ${DOMAIN}_0_0.html
	
}

findDomainAndMoveIn() {

	extractDomain
	set -- $( head -1 temp1 )
	DOMAIN=$( echo $1 ) # Define domain for analysis
	rm temp1
	[ ! -d ${DOMAIN}folder ] && mkdir ${DOMAIN}folder
	cd ${DOMAIN}folder
	
}

downloadManager() {

	URLS=$( head -1 urlArguments )
	set -- $URLS
	
	for i in $*; do 
	
		URL=$( echo $i )
		findDomainAndMoveIn
		downloadIndex
		[ $LEVEL -gt 0 ] && downloadChildren
		cd ..

	done

	cd ..
	
}	

findGenerator() { # If GENERATOR = 1 then generator exists and use CONTENT

	grep "name= generator" < $FILENAME > temp1
	GENERATOR=$( grep "name= generator" temp1 | wc -l )
	[ "$GENERATOR" -eq "1" ] && echo "<li>Generator used is $( set -- $( grep -o 'content=.*' temp1 ) | echo $2 )</li>" >> ${FILEANALYSED}.txt
	[ "$GENERATOR" -eq "1" ] || echo "<li>No generator used</li>" >> ${FILEANALYSED}.txt
	# [ $( echo $CONTENT | wc -w) -gt 0 ] && CONTENT=$( sed "s/.*content='\(*\)'.*/\1/" )
	rm temp1
}

findGoogleAnalytics() {

	if [ $( grep "https://www.google-analytics.com/urchin.js" < $FILENAME | wc -l ) -gt 0 ]; then 
		echo "<li>Google analytics used</li>" >> ${FILEANALYSED}.txt
	else
		echo "<li>Google analytics is not used</li>" >> ${FILEANALYSED}.txt
	fi

}

findExternalScripts() {

	EXTERNALSCRIPTCOUNT=0
	EXTERNALSCRIPTCOUNT=$( grep 'src' < SCRIPTS | wc -l )
	echo "<li>Refers $EXTERNALSCRIPTCOUNT external script(s)</li>" >> ${FILEANALYSED}.txt
}

findInternalScripts() {

	INTERNALSCRIPTCOUNT=0
	INTERNALSCRIPTCOUNT=$( grep -v '<script.*src.*>' < SCRIPTS | wc -l )
	echo "<li>Defines $INTERNALSCRIPTCOUNT internal script(s)</li>" >> ${FILEANALYSED}.txt

}

findScripts() {

	SCRIPTCOUNT=0
	SCRIPTCOUNT=$( grep '<script' < $FILENAME | wc -l )

	if [ $SCRIPTCOUNT -gt 0 ]; then  
		touch SCRIPTS
		echo $( grep 'script' < $FILENAME | grep -v '</script>' ) > SCRIPTS
		echo "<li>Defines $SCRIPTCOUNT script(s)</li>" >> ${FILEANALYSED}.txt
 		findExternalScripts 
		findInternalScripts
		rm SCRIPTS
	else
		echo "<li>No scripts defined</li>" >> ${FILEANALYSED}.txt
	fi
	
}

findExternalCss() {

	EXTERNALCSSCOUNT=0
	EXTERNALCSSCOUNT=$( grep 'stylesheet' $FILENAME | wc -l )
	echo "<li>Refers $EXTERNALCSSCOUNT external css file(s)</li>" >> ${FILEANALYSED}.txt

}

findStyle() {

	STYLECOUNT=0
	STYLECOUNT=$( grep '<style.*>' $FILENAME | wc -l )
	echo "<li>Defines $STYLECOUNT style rules</li>" >> ${FILEANALYSED}.txt

}

findInternalCss() {

	INTERNALCSSCOUNT=0
	INTERNALCSSCOUNT=$( tr "\n" "|" < $FILENAME | grep -o '<style>.*</style>' | sed 's/\(<style>\|<\/style>\)//g' | sed 's/|/\n/g' | wc -l
)
	echo "<li>Defines $INTERNALCSSCOUNT internal CSS(s)</li>" >> ${FILEANALYSED}.txt

}	

findInlineCss() {

	INLINECSSCOUNT=0
	INLINECSSCOUNT=$( grep -o 'style=' $FILENAME | wc -l )
	echo "<li>Defines $INLINECSSCOUNT inline CSS rule(s)</li>" >> ${FILEANALYSED}.txt

}

findMeta() {

	METACOUNT=0
	METACOUNT=$( grep '<meta' $FILENAME | wc -l )
	echo "<li>Defines $METACOUNT metadata</li>" >> ${FILEANALYSED}.txt

}

findIFrame() {

	IFRAMECOUNT=0
	IFRAMECOUNT=$( grep '<iframe' $FILENAME | wc -l )
	echo "<li>Uses $IFRAMECOUNT iframe(s)</li>" >> ${FILEANALYSED}.txt

}

countTags() {

	while read tag; do
		TAGTOANALYSE=$( echo $tag )
		countSpecificTag
	done < uniqueTagsSorted.txt

}

getTags() {

	grep -o '<[a-z].*>' $FILENAME > tagscraper

	while read tag; do
		set -- $tag
		echo $1 >> tagsUnsorted
	done < tagscraper

	sed 's/</ /g' tagsUnsorted | sed 's/>/ /g' > tagsSpace.txt
	rm tagsUnsorted

	while read tag; do
		set -- $tag
		echo $1 >> tagsSorted.txt
	done < tagsSpace.txt

	sort -u TagsSorted.txt > uniqueTagsSorted.txt
	rm tagsSorted.txt

	countTags
	
}

countSpecificTag() {

	OPENINGTAG=$( grep "<${TAGTOANALYSE}" $FILENAME | wc -l )
	ClOSINGTAG=$( grep "/${TAGTOANALYSE}>" $FILENAME | wc -l )
	EMPTYTAG=$( grep "<${TAGTOANALYSE} *\/ *>" $FILENAME | wc -l )
	HTMLROW="<tr style='border: 1px solid black'><td style='border: 1px solid black'>${TAGTOANALYSE}</td><td style='border: 1px solid black'>${OPENINGTAG}</td><td style='border: 1px solid black'>${ClOSINGTAG}</td><td style='border: 1px solid black'>${EMPTYTAG}</td></tr>"
	echo $HTMLROW >> ${FILEANALYSED}.txt

}

analyseTags() {

	set -- $( head -1 tags )

	for i in $*; do
		TAGTOANALYSE=$( echo $i )
		countSpecificTag
	done
	
}

tableHead() {

	echo "<table style='border: 1px solid black;'>" >> ${FILEANALYSED}.txt
	echo "<thead style='border: 1px solid black>" >> ${FILEANALYSED}.txt
	echo "<tr style='border: 1px solid black'><th style='border: 1px solid black'>tag</th><th style='border: 1px solid black'>open count</th><th style='border: 1px solid black'>closed count</th><th style='border: 1px solid black'>empty count</th></tr>" >> ${FILEANALYSED}.txt
	echo "</thead>" >> ${FILEANALYSED}.txt
	echo "<tbody>" >> ${FILEANALYSED}.txt

}

analyse() {

	URLS=$( head -1 urlArguments )
	set -- $URLS

	for i in $*; do

		URL=$( echo $i )	
		findDomainAndMoveIn # Found by interpreting current $URL
		
		# Analyse all downloaded files in folder
		ls ${DOMAIN}* > pagesToAnalyse
		sed 's/\(.*\)\.html/\1/' pagesToAnalyse > pagesToCreate

		LINENUMBER=1
		while read page; do
			
			FILENAME=$( echo $page )
			sed "${LINENUMBER}q;d" pagesToCreate > line
			FILEANALYSED=$( head -1 line )
			rm line
			touch ${FILEANALYSED}.txt
			HTMLURL=$( echo $( tail -1 < $page ) )

			TITLEBEGIN='<h2><a href="'
			TITLEBEGIN+=$HTMLURL
			TITLEBEGIN+='">'
			TITLEBEGIN+=$HTMLURL
			TITLEBEGIN+='</a></h2>'

			echo $TITLEBEGIN >> ${FILEANALYSED}.txt
			echo "<ul>" >>	${FILEANALYSED}.txt

			findGenerator
			findGoogleAnalytics
			findScripts
			findStyle
			findExternalCss
			findInternalCss
			findInlineCss
			findMeta
			findIFrame

			echo "</ul>" >> ${FILEANALYSED}.txt
			
			cd ..
			cp tags ${DOMAIN}folder
			cd ${DOMAIN}folder

			TAGS=$( head -1 tags )
			set -- $TAGS

			if [ "$( echo $TAGS )" != "" ]; then 
				tableHead
				if [ "$( echo $1 )" == "all" ]; then
					getTags
				else
					analyseTags
				fi
			fi

			echo "</tbody>" >> ${FILEANALYSED}.txt
			echo "</table>" >> ${FILEANALYSED}.txt

			mv ${FILEANALYSED}.txt ../analysedPages

			LINENUMBER=$( expr $LINENUMBER + 1 )

		done < pagesToAnalyse

		cd ..

	done

	cd ..
	
}


create() {

	cd downloads/analysedPages
	touch index.html

	echo "<!doctype html><html><head><meta charset='utf8' /><title>Web Page Analysis</title></head>" >> index.html
	echo "<body>" >> index.html
	echo "<h1>Statistics</h1>" >> index.html

	for i in *.txt; do
		FILENAME=$( echo $i )
		cat $FILENAME >> index.html
	done

	echo "</body></html>" >> index.html

	mv index.html ../../

}


case $1 in
-z) init;;
-c) remove;;
-[0-9]) LEVEL=$( echo $1 | tail -c 2 ); shift; cd downloads; touch urlArguments; echo $* > urlArguments; downloadManager;;
-a) shift; cd downloads; touch tags; echo $* > tags; analyse;;
-h) create;;
esac
